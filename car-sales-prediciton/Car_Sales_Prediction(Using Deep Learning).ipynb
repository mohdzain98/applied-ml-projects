{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7563fb18",
   "metadata": {},
   "source": [
    "# Cars Sales Prediction Using Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b2c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
    "plt.rcParams['figure.figsize'] = 6, 4\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb7cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac9dbc8",
   "metadata": {},
   "source": [
    "**Data Analysing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Audit\n",
    "def continuous_var_summary(x):\n",
    "    return pd.Series([x.count(), x.isnull().sum(), x.sum(), x.mean(), x.median(),  \n",
    "                      x.std(), x.var(), x.min(), x.quantile(0.01), x.quantile(0.05),\n",
    "                          x.quantile(0.10),x.quantile(0.25),x.quantile(0.50),x.quantile(0.75), \n",
    "                              x.quantile(0.90),x.quantile(0.95), x.quantile(0.99),x.max()], \n",
    "                  index = ['N', 'NMISS', 'SUM', 'MEAN','MEDIAN', 'STD', 'VAR', 'MIN', 'P1', \n",
    "                               'P5' ,'P10' ,'P25' ,'P50' ,'P75' ,'P90' ,'P95' ,'P99' ,'MAX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_var_summary(x):\n",
    "    Mode = x.value_counts().sort_values(ascending = False)[0:1].reset_index()\n",
    "    return pd.Series([x.count(), x.isnull().sum(), Mode.iloc[0, 0], Mode.iloc[0, 1], \n",
    "                          round(Mode.iloc[0, 1] * 100/x.count(), 2)], \n",
    "                  index = ['N', 'NMISS', 'MODE', 'FREQ', 'PERCENT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3a53a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value imputation for categorical and continuous variables\n",
    "def missing_imputation(x, stats = 'mean'):\n",
    "    if (x.dtypes == 'float64') | (x.dtypes == 'int64'):\n",
    "        x = x.fillna(x.mean()) if stats == 'mean' else x.fillna(x.median())\n",
    "    else:\n",
    "        x = x.fillna(x.mode())\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6074ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An utility function to create dummy variable\n",
    "def create_dummies(df, colname):\n",
    "    col_dummies = pd.get_dummies(df[colname], prefix = colname, drop_first = True)\n",
    "    df = pd.concat([df, col_dummies], axis = 1)\n",
    "    df.drop(colname, axis = 1, inplace = True )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632e6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_excel('car_sales.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a65272",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d493749",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ec8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_conti_vars = cars.loc[:, (cars.dtypes == 'float64') | (cars.dtypes == 'int64')]\n",
    "cars_cat_vars = cars.loc[:, (cars.dtypes == 'object')]\n",
    "#cars_cat_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2358649",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_conti_vars.apply(continuous_var_summary).T.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a54bc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_cat_vars.apply(categorical_var_summary).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a42b79",
   "metadata": {},
   "source": [
    "**Outlier Treatment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea617e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_conti_vars = cars_conti_vars.apply(lambda x: x.clip(lower = x.quantile(0.01), upper = x.quantile(0.99)))\n",
    "cars_conti_vars.apply(continuous_var_summary).T.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be1ab9",
   "metadata": {},
   "source": [
    "**Missing Value Treatment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9719be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_conti_vars = cars_conti_vars.apply(missing_imputation)\n",
    "cars_cat_vars = cars_cat_vars.apply(missing_imputation)\n",
    "cars_conti_vars.apply(continuous_var_summary).T.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43337971",
   "metadata": {},
   "source": [
    "**Dealing Categorical Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4f5429",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_cat_vars.Manufacturer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4453f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_cat_vars = cars[['Manufacturer', 'Vehicle_type']]\n",
    "\n",
    "for c_feature in ['Manufacturer', 'Vehicle_type']:\n",
    "    cars_cat_vars[c_feature] = cars_cat_vars[c_feature].astype('category')\n",
    "    cars_cat_vars = create_dummies(cars_cat_vars, c_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5fa466",
   "metadata": {},
   "source": [
    "**Final Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f38c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_new = pd.concat([cars_conti_vars, cars_cat_vars], axis = 1)\n",
    "cars_new.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588f2328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of variables\n",
    "sns.distplot(cars_new.Sales_in_thousands)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1363b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply log transformation: log is rescalling the data and making the distribution normal\n",
    "cars_new['ln_sales_in_thousands'] = np.log(cars_new['Sales_in_thousands']+1)\n",
    "\n",
    "# Distribution of variables\n",
    "sns.distplot(cars_new.ln_sales_in_thousands)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138f6c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d4d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_new.drop(['Power_perf_factor','__year_resale_value'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf0f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the variables based low correlation with Y\n",
    "#cars_new.drop(['four_year_resale_value', 'Power_perf_factor'], axis = 1, inplace = True)\n",
    "cars_new.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2c7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf0b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data: separate out the feature/input/independant columns and dependant variable\n",
    "#cars_new.columns\n",
    "feature_columns = cars_new.columns.difference(['ln_sales_in_thousands', 'Sales_in_thousands'])\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bea419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separate Target Variable and Predictor Variables\n",
    "TargetVariable=['ln_sales_in_thousands']\n",
    " \n",
    "X=cars_new.drop(columns=['ln_sales_in_thousands','Sales_in_thousands'])\n",
    "y=cars_new[TargetVariable].values\n",
    " \n",
    "### Sandardization of data ###\n",
    "PredictorScaler=StandardScaler()\n",
    "TargetVarScaler=StandardScaler()\n",
    " \n",
    "# Storing the fit object for later reference\n",
    "PredictorScalerFit=PredictorScaler.fit(X)\n",
    "TargetVarScalerFit=TargetVarScaler.fit(y)\n",
    " \n",
    "# Generating the standardized values of X and y\n",
    "X=PredictorScalerFit.transform(X)\n",
    "y=TargetVarScalerFit.transform(y)\n",
    " \n",
    "# Split the data into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    " \n",
    "# Quick sanity check with the shapes of Training and testing datasets\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9272b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "LR = LinearRegression()\n",
    "LR.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6b08fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred =  LR.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836a70df",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae=mean_absolute_error(y_test,y_pred)\n",
    "mse=mean_squared_error(y_test,y_pred)\n",
    "r2=r2_score(y_test,y_pred)\n",
    "print(\"Mean Absolute Error:\",mae)\n",
    "print(\"Mean Squared Error:\",mse)\n",
    "print(\"R2 Score:\",r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c0584",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors=pd.DataFrame(data=cars_new,columns=cars_new.drop(columns=['ln_sales_in_thousands','Sales_in_thousands']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c78c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions=TargetVarScalerFit.inverse_transform(y_pred)\n",
    " \n",
    "# Scaling the y_test Price data back to original price scale\n",
    "y_test_orig=TargetVarScalerFit.inverse_transform(y_test)\n",
    " \n",
    "# Scaling the test data back to original scale\n",
    "LRTest_Data=PredictorScalerFit.inverse_transform(X_test)\n",
    " \n",
    "LRTestingData=pd.DataFrame(data=LRTest_Data, columns=predictors.columns)\n",
    "LRTestingData['Value']=y_test_orig\n",
    "LRTestingData['PredictedValue']=Predictions\n",
    "LRTestingData.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8e5b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "LRresult=pd.DataFrame({'Actual':LRTestingData['Value'],'Predicted':LRTestingData['PredictedValue']})\n",
    "LRresult['Actual']=np.exp(LRresult['Actual'])\n",
    "LRresult['Predicted']=np.exp(LRresult['Predicted'])\n",
    "LRresult=LRresult.astype({'Actual':float,'Predicted':float})\n",
    "\n",
    "LRresult.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c33b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##applying deep learning\n",
    "# importing the libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    " \n",
    "# create ANN model\n",
    "model = Sequential()\n",
    " \n",
    "# Defining the Input layer \n",
    "model.add(Dense(units=5, input_dim=28, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "# The output neuron is a single fully connected node \n",
    "# Since we will be predicting a single number\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    " \n",
    "# Compiling the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    " \n",
    "# Fitting the ANN to the Training set\n",
    "model.fit(X_train, y_train ,batch_size =10, epochs = 100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87332071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Predictions on testing data\n",
    "Predictions=model.predict(X_test)\n",
    " \n",
    "# Scaling the predicted Price data back to original price scale\n",
    "Predictions=TargetVarScalerFit.inverse_transform(Predictions)\n",
    " \n",
    "# Scaling the y_test Price data back to original price scale\n",
    "y_test_orig=TargetVarScalerFit.inverse_transform(y_test)\n",
    " \n",
    "# Scaling the test data back to original scale\n",
    "Test_Data=PredictorScalerFit.inverse_transform(X_test)\n",
    " \n",
    "TestingData=pd.DataFrame(data=Test_Data, columns=predictors.columns)\n",
    "TestingData['Value']=y_test_orig\n",
    "TestingData['PredictedValue']=Predictions\n",
    "TestingData.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b5ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
    "print('Accuracy of the model is',r2_score(y_test_orig,Predictions))\n",
    "print('MSE',mean_squared_error(y_test_orig,Predictions))\n",
    "print('MAE',mean_absolute_error(y_test_orig,Predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fdd955",
   "metadata": {},
   "outputs": [],
   "source": [
    "fresult=pd.DataFrame({'Actual':TestingData['Value'],'Predicted':TestingData['PredictedValue']})\n",
    "fresult['Actual']=np.exp(fresult['Actual'])\n",
    "fresult['Predicted']=np.exp(fresult['Predicted'])\n",
    "fresult=fresult.astype({'Actual':float,'Predicted':float})\n",
    "\n",
    "fresult.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#both have same actual values\n",
    "Gresult=pd.DataFrame({'Actual(All in thousands)':LRresult.Actual,'Linear Regression.Predicted':LRresult.Predicted,\n",
    "                      'Neural Network.Predicted':fresult.Predicted})\n",
    "Gresult.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0202daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "f.set_figwidth(8)\n",
    "f.set_figheight(8)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "sns.lineplot(x=LRresult['Actual'],y=LRresult['Predicted'],label='Linear Regression')\n",
    "sns.lineplot(x=fresult['Actual'],y=fresult['Predicted'],label='Neural Network')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
